- Application teams have the freedom to overwrite OE container configurations, leading to inconsistencies across clusters and namespaces.  
- Lack of centralized control over OE container versions, resource configurations, and deployments.  
- Broad rollbacks during incidents (e.g., **INCIDENT-17594**) require reverting changes across all roles and namespaces, even for unaffected applications.  
- No granularity in rollbacks or updates, making it difficult to isolate faulty applications and avoid unnecessary downtime.  
- Manual updates and configurations are time-consuming, error-prone, and require coordination with multiple application teams.  
- Inefficient incident resolution processes prolong Mean Time to Resolve (MTTR) due to decentralized configuration management.  
- Delayed adoption of updates by application teams leads to version fragmentation and inconsistent observability setups.  
- Resource mismanagement due to application teams customizing resource requests and limits without OE team oversight.  
- Scaling challenges in multi-cluster environments where configurations are not centrally managed, increasing the risk of inconsistencies.  
- Limited visibility into the deployment and health status of OE containers across all clusters.  
- Reactive incident management due to lack of real-time monitoring and centralized control over observability containers.  
- Amplified impact of configuration errors as misconfigured OE containers affect multiple clusters or namespaces.  
- High operational overhead for the OE team to coordinate changes and ensure compliance across all application teams.  
- Lack of enforced policies to standardize OE container configurations, leading to unpredictable behaviors in the observability infrastructure.  
- Difficulty in maintaining observability reliability during scaling events or new application deployments.
